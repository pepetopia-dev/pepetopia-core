import google.generativeai as genai
from google.api_core import exceptions
import logging
import re
from src.app_config import Config

logger = logging.getLogger(__name__)

# --- SYSTEM PROMPT (X ALGORITHM EXPERT) ---
SYSTEM_PROMPT = """
You are an expert X (Twitter) Algorithm Strategist for 'Pepetopia'.
Your Goal: Analyze the tweet and generate a high-ranking reply strategy based on the 'Heavy Ranker' algorithm.

INTERNAL RULES:
1. SimClusters: Ensure relevance to the topic.
2. Reply Weight (75x): End with a question or hook.
3. Media Decision: Suggest Video (>3s) for emotion, Image for data.
4. Safety: ZERO toxicity.
5. NO LINKS.

OUTPUT FORMAT:
- STRATEGY: Why this angle? Media choice?
- DRAFT: The tweet text (Max 280 chars).
- SCORE: Predicted Algo Score (0-100).
"""

class ModelManager:
    """
    Manages dynamic discovery and fallback logic for Gemini models.
    """
    _cached_models = []

    @staticmethod
    def _extract_version(model_name: str) -> float:
        """
        Extracts version number for sorting. 
        Supports both '1.5' (float) and '3' (int) formats.
        """
        # Regex Update: (\d+(?:\.\d+)?) matches "3" and "2.5"
        match = re.search(r'(\d+(?:\.\d+)?)', model_name)
        if match:
            return float(match.group(1))
        return 0.0
    
    @staticmethod
    def get_prioritized_models():
        """
        Fetches models from Google, filters for 'generateContent', 
        and sorts by Version (Newest First) -> Name.
        """
        # If we already fetched the list, use cache to save time
        if ModelManager._cached_models:
            return ModelManager._cached_models

        try:
            logger.info("üì° Discovering available Gemini models...")
            genai.configure(api_key=Config.GEMINI_API_KEY)
            
            all_models = list(genai.list_models())
            valid_models = []

            for m in all_models:
                # Filter: Must be 'gemini' and support text generation
                if 'generateContent' in m.supported_generation_methods and 'gemini' in m.name:
                    valid_models.append(m.name)
            
            # SORTING LOGIC:
            # 1. Sort by version descending (2.0 > 1.5)
            # 2. Prefer 'pro' over 'flash' if versions are equal (heuristic)
            valid_models.sort(key=lambda x: (ModelManager._extract_version(x), 'pro' in x), reverse=True)
            
            if not valid_models:
                # Fallback if API list fails
                logger.warning("‚ö†Ô∏è No models found dynamically. Using hardcoded fallback.")
                return ["models/gemini-1.5-pro", "models/gemini-1.5-flash"]

            logger.info(f"‚úÖ Model Priority List: {valid_models}")
            ModelManager._cached_models = valid_models
            return valid_models

        except Exception as e:
            logger.error(f"‚ùå Model discovery failed: {e}")
            return ["models/gemini-1.5-flash"]

def analyze_and_draft(user_input: str) -> str:
    """
    Tries to generate content using the best available model.
    If Rate Limit hits, falls back to the next model in the list.
    """
    models = ModelManager.get_prioritized_models()
    
    final_prompt = f"""
    Target Tweet/Context:
    "{user_input}"
    
    Task: Apply X Algorithm Checklist and draft a strategy.
    """

    # --- FALLBACK LOOP ---
    for model_name in models:
        try:
            # logger.info(f"üß† Attempting with engine: {model_name}") 
            
            # Configure and generate
            model = genai.GenerativeModel(
                model_name, 
                system_instruction=SYSTEM_PROMPT
            )
            
            response = model.generate_content(final_prompt)
            
            # If successful, add a footer showing which brain was used (for transparency)
            result = response.text.strip()
            result += f"\n\n‚öôÔ∏è *Generated by: {model_name.replace('models/', '')}*"
            
            return result

        except exceptions.ResourceExhausted:
            logger.warning(f"‚ö†Ô∏è Quota Exceeded for {model_name}. Switching to next engine...")
            continue # Try next model
            
        except Exception as e:
            logger.error(f"‚ùå Error with {model_name}: {e}. Trying next...")
            continue # Try next model

    return "üö´ CRITICAL ERROR: All AI models failed. Please check API Key or Quota."